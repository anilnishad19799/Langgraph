{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b36fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain.chat_models import init_chat_model\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from PIL import Image\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from mem0 import Memory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "from langgraph.graph import StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96428a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7924adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        # \"math\": {\n",
    "        #     \"command\": \"python\",\n",
    "        #     # Make sure to update to the full absolute path to your math_server.py file\n",
    "        #     \"args\": [\"math_server.py\"],\n",
    "        #     \"transport\": \"stdio\",\n",
    "        # },\n",
    "        # \"weather\": {\n",
    "        #     \"command\": \"python\",\n",
    "        #     # Make sure to update to the full absolute path to your math_server.py file\n",
    "        #     \"args\": [\"weather_server.py\"],\n",
    "        #     \"transport\": \"stdio\",\n",
    "        # },\n",
    "\n",
    "        \"math\": {\n",
    "            # make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8001/mcp\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            # make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8002/mcp\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = await client.get_tools()\n",
    "load_dotenv()\n",
    "checkpointer = InMemorySaver()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3f7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "daae6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"Conversation state passed between nodes\"\"\"\n",
    "    messages: Annotated[list[BaseMessage], add_messages]  # chat history for this request\n",
    "    mem0_user_id: str     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fa5697f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='mem0migrations'), CollectionDescription(name='mem0_yt')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=42, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# config = {\n",
    "#     \"history_db_path\": \"history.db\",\n",
    "#     \"llm\": {\n",
    "#         \"provider\": \"openai\",\n",
    "#         \"config\": {\n",
    "#             \"model\": \"gpt-4.1-mini\",\n",
    "#             \"temperature\": 0.2,\n",
    "#             \"max_tokens\": 2000\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# memory = Memory.from_config(config)\n",
    "\n",
    "# now use qdrant memory\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://a92a20e5-49ff-4a82-8365-49bdb11ce639.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.KlvM6TDdSV_v3rZj9hQh5vvsEGOJNrsx8TThjp1N7OA\",\n",
    ")\n",
    "\n",
    "print(qdrant_client.get_collections())\n",
    "\n",
    "from mem0 import Memory\n",
    "collection_name = \"mem0_yt\"\n",
    "userdata = {\"Qdrant_API_KEY\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.KlvM6TDdSV_v3rZj9hQh5vvsEGOJNrsx8TThjp1N7OA\"}\n",
    "\n",
    "config = {\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"qdrant\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": collection_name,\n",
    "            \"host\": \"a92a20e5-49ff-4a82-8365-49bdb11ce639.us-east4-0.gcp.cloud.qdrant.io\",\n",
    "            \"port\": 6333,\n",
    "            \"api_key\": userdata.get(\"Qdrant_API_KEY\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "memory = Memory.from_config(config)\n",
    "\n",
    "# Mem0 filters by user_id when searching, so Qdrant needs a keyword index on that field. If you skip this step youâ€™ll get:\n",
    "# 400 Bad Request â€“ Index required but not found for \"user_id\" of type [keyword]\n",
    "qdrant_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"user_id\",\n",
    "    field_schema=\"keyword\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7a19c",
   "metadata": {},
   "source": [
    "## using init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e03aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\"openai:gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8e6038c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_model(state: State):\n",
    "    global memory\n",
    "    msgs = state[\"messages\"]\n",
    "    uid = state[\"mem0_user_id\"]\n",
    "\n",
    "    # Retrieve top 2 relevant memories only once\n",
    "    mems = memory.search(msgs[-1].content, user_id=uid, limit=2, rerank=True)\n",
    "    context = \"\\n\".join(f\"- {m['memory']}\" for m in mems[\"results\"]) if mems[\"results\"] else \"\"\n",
    "\n",
    "    # System prompt to guide tool usage\n",
    "    system = SystemMessage(content=f\"\"\"\n",
    "    You are a helpful assistant.\n",
    "    Tools available:\n",
    "    1. Math tool â€“ for calculations.\n",
    "    2. Weather tool â€“ for weather queries.\n",
    "    Use tools only when needed; otherwise respond naturally.\n",
    "    Memory context:\n",
    "    {context}\n",
    "    \"\"\")\n",
    "\n",
    "    # Await async tool calls\n",
    "    print(\"context\", context)\n",
    "    print(\"[system] + msgs\", [system] + msgs)\n",
    "    response = await model.bind_tools(tools).ainvoke([system] + msgs)\n",
    "\n",
    "    # Persist memory\n",
    "    memory.add([\n",
    "        {\"role\": \"user\", \"content\": msgs[-1].content},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ], user_id=uid)\n",
    "\n",
    "    print(\"*\"*100)\n",
    "    print(\"response\",response)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2e5a7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_model(state: MessagesState):\n",
    "#     response = model.bind_tools(tools).invoke(state[\"messages\"])\n",
    "#     return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ba9b3",
   "metadata": {},
   "source": [
    "## using creat_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "360099aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class WeatherInfo(BaseModel):\n",
    "    \"\"\"Contact information for a person.\"\"\"\n",
    "    city: str = Field(description=\"City name in query\")\n",
    "    weather_status: str = Field(description=\"Actual weather status\")\n",
    "\n",
    "class MathInfo(BaseModel):\n",
    "    \"\"\"Contact information for a person.\"\"\"\n",
    "    problem: str = Field(description=\"Actual Math problem given by user \")\n",
    "    output: str = Field(description=\"Actual output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ToolStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "67bd06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from typing import Union\n",
    "model = create_agent(\n",
    "    model=\"gpt-4.1-mini\", \n",
    "    tools=tools,\n",
    "    response_format=ToolStrategy(Union[WeatherInfo, MathInfo]) \n",
    "    # response_format=WeatherInfo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871fa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "eb67ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_model(state: dict):\n",
    "    global memory\n",
    "    msgs = state[\"messages\"]\n",
    "    uid = state[\"mem0_user_id\"]\n",
    "\n",
    "    # Retrieve top 2 relevant memories\n",
    "    mems = memory.search(msgs[-1].content, user_id=uid, limit=2, rerank=True)\n",
    "    context = \"\\n\".join(f\"- {m['memory']}\" for m in mems[\"results\"]) if mems[\"results\"] else \"\"\n",
    "\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "You are a helpful assistant.\n",
    "Tools available:\n",
    "1. Math tool â€“ for calculations.\n",
    "2. Weather tool â€“ for weather queries.\n",
    "Use tools only when needed; otherwise respond naturally.\n",
    "Memory context:\n",
    "{context}\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    user_message = {\"role\": \"user\", \"content\": msgs[-1].content}\n",
    "    print(\"user_message\", user_message)\n",
    "    print(\"msgs\", msgs)\n",
    "\n",
    "    # Invoke model\n",
    "    response = await model.ainvoke({\n",
    "    \"messages\": [system_message,\n",
    "                 user_message]\n",
    "})\n",
    "\n",
    "    # Persist memory\n",
    "    memory.add([\n",
    "        {\"role\": \"user\", \"content\": user_message[\"content\"]},\n",
    "        {\"role\": \"assistant\", \"content\": response['messages'][-1].content}\n",
    "    ], user_id=uid)\n",
    "\n",
    "    # Return a dict (LangGraph expects a dict)\n",
    "    return {\"messages\": [response['messages'][-1]]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861407e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ec98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e35a4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder = StateGraph(MessagesState)\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(call_model)\n",
    "graph_builder.add_node(ToolNode(tools))\n",
    "graph_builder.add_edge(START, \"call_model\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"call_model\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"call_model\")\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "44fa757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "dc2213ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math_response = await graph.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "# print(\"Math Response:\", math_response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "47c72db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5e27707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "56db073f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_message {'role': 'user', 'content': 'what is 2*2*2'}\n",
      "msgs [HumanMessage(content='what is weather at new york', additional_kwargs={}, response_metadata={}, id='0e2e228b-6081-4f04-9c51-3e272966106e'), AIMessage(content='{\"city\":\"New York\",\"weather_status\":\"It\\'s always sunny in New York\"}', additional_kwargs={'parsed': None, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 225, 'total_tokens': 246, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CWhv9KDNPuAA49LvLIM2iHNT4H41V', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--d84a4b4a-2c42-4e4f-a61b-391b7d5fc79d-0', usage_metadata={'input_tokens': 225, 'output_tokens': 21, 'total_tokens': 246, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is 2*2*2', additional_kwargs={}, response_metadata={}, id='f380aa13-cfa1-47d2-8b9f-78b3366fcf1a')]\n",
      "ðŸ¤– Returning structured response: problem='2*2*2' output='8'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "async def run_conversation(user_input: str, mem0_user_id: str):\n",
    "    state = {\"messages\": [HumanMessage(content=user_input)], \"mem0_user_id\": mem0_user_id}\n",
    "    result = await graph.ainvoke(state, config=config)\n",
    "    print(\"ðŸ¤–\", result[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uid = \"suraj\"\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() in {\"quit\", \"exit\", \"bye\"}:\n",
    "            break\n",
    "        await run_conversation(inp, uid)  # use await instead of direct call\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20662fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "63167b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CWhX3NAGN8xujVMgFSOWZojUB0kge', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--e7177659-f05d-4041-9d93-7d15d328ecee-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tools = await client.get_tools()\n",
    "load_dotenv()\n",
    "checkpointer = InMemorySaver()  \n",
    "init_model = init_chat_model(\"openai:gpt-3.5-turbo\")\n",
    "init_model.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "937ec0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "model = create_agent(model=\"gpt-4.1-mini\")\n",
    "output = model.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "88d7f8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc3774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a95ab5",
   "metadata": {},
   "source": [
    "### Testing Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "41f4f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import PIIMiddleware\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    # tools=[customer_service_tool, email_tool],\n",
    "    middleware=[\n",
    "        # Redact emails in user input before sending to model\n",
    "        PIIMiddleware(\n",
    "            \"email\",\n",
    "            strategy=\"redact\",\n",
    "            apply_to_input=True,\n",
    "        ),\n",
    "        # Mask credit cards in user input\n",
    "        PIIMiddleware(\n",
    "            \"credit_card\",\n",
    "            strategy=\"mask\",\n",
    "            apply_to_input=True,\n",
    "        ),\n",
    "        # Block API keys - raise error if detected\n",
    "        PIIMiddleware(\n",
    "            \"api_key\",\n",
    "            detector=r\"sk-[a-zA-Z0-9]{32}\",\n",
    "            strategy=\"block\",\n",
    "            apply_to_input=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# When user provides PII, it will be handled according to the strategy\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"My email is john.doe@example.com and card is 4532-1234-5678-9010\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5b65bbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I can't assist with that request.\""
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "439d8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    # tools=[search_tool, send_email_tool, delete_database_tool],\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                # Require approval for sensitive operations\n",
    "                \"send_email\": True,\n",
    "                \"delete_database\": True,\n",
    "                # Auto-approve safe operations\n",
    "                \"search\": False,\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    "    # Persist the state across interrupts\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "# Human-in-the-loop requires a thread ID for persistence\n",
    "config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
    "\n",
    "# Agent will pause and wait for approval before executing sensitive tools\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "74c612a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Below is a template for sending an email to your team. Feel free to customize it to fit your specific needs and context:\\n\\n---\\n\\nSubject: [Your Topic/Announcement]\\n\\nHi Team,\\n\\nI hope this message finds you well. I wanted to take a moment to [share some updates/discuss an important topic/announce something significant].\\n\\n[Include details about the topic. Be clear and concise. If it's an update, explain what's happened and its implications. If it's a discussion, outline the key points and invite input.]\\n\\nKey Points:\\n- [Point 1]\\n- [Point 2]\\n- [Point 3]\\n\\nPlease let me know if you have any questions or need further clarification. [If applicable, include any deadlines, calls to action, or requests for feedback.]\\n\\nThank you all for your hard work and dedication. Looking forward to [working on this together/moving forward with this initiative].\\n\\nBest regards,\\n\\n[Your Name]\\n\\n[Your Position]\\n\\n[Your Contact Information]\\n\\n---\\n\\nAdjust the placeholders and content to align with your specific situation.\""
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = agent.invoke(\n",
    "    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n",
    "    config=config  # Same thread ID to resume the paused conversation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
