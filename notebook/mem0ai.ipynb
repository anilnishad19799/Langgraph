{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7896bab",
   "metadata": {},
   "source": [
    "### Whole code is from blog futuresmartAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951eef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mem0‚ÄØBasics ‚Äì Adding, Updating & Searching Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31482dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abea9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"üîë Enter your OpenAI API key: \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5defaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import Memory\n",
    "# memory = Memory.from_config({\"history_db_path\": \"history.db\"})  # local SQLite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf945b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"history_db_path\": \"history.db\",\n",
    "    \"llm\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"gpt-4.1-mini\",\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "    }\n",
    "}\n",
    "memory = Memory.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593330fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': 'a7885191-1db0-45b8-93a2-3a4cdde4a9ff',\n",
       "   'memory': 'Name is Pradip Nichite',\n",
       "   'event': 'ADD'},\n",
       "  {'id': 'c1159f6f-9743-4a67-a893-d0aa10b280ee',\n",
       "   'memory': 'Runs FutureSmart AI',\n",
       "   'event': 'ADD'},\n",
       "  {'id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       "   'memory': 'Builds custom AI solutions',\n",
       "   'event': 'ADD'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.add([\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I'm Pradip Nichite. I run FutureSmart AI, where we build custom AI solutions.\"}\n",
    "], user_id=\"pradip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dbe9720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       "   'memory': 'Loves building RAG and AI Agent solutions that work in production',\n",
       "   'event': 'UPDATE',\n",
       "   'previous_memory': 'Builds custom AI solutions'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.add([\n",
    "    {\"role\": \"user\", \"content\": \"I love building RAG and AI Agent solutions that actually work in production.\"}\n",
    "], user_id=\"pradip\", metadata={\"category\": \"preferences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451603fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': 'a7885191-1db0-45b8-93a2-3a4cdde4a9ff',\n",
       "   'memory': 'Name is Pradip Nichite',\n",
       "   'hash': 'fa942a6331bb89da286d4a9e296d1008',\n",
       "   'metadata': None,\n",
       "   'score': 0.22956096244058533,\n",
       "   'created_at': '2025-10-27T03:15:38.955725-07:00',\n",
       "   'updated_at': None,\n",
       "   'user_id': 'pradip'},\n",
       "  {'id': 'c1159f6f-9743-4a67-a893-d0aa10b280ee',\n",
       "   'memory': 'Runs FutureSmart AI',\n",
       "   'hash': '68a143a88a3e67ae9ebfb9575bcf49a7',\n",
       "   'metadata': None,\n",
       "   'score': 0.1551708411081418,\n",
       "   'created_at': '2025-10-27T03:15:38.967325-07:00',\n",
       "   'updated_at': None,\n",
       "   'user_id': 'pradip'},\n",
       "  {'id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       "   'memory': 'Loves building RAG and AI Agent solutions that work in production',\n",
       "   'hash': '93e19ea42b4fd49fa0041025b13429cd',\n",
       "   'metadata': {'category': 'preferences'},\n",
       "   'score': 0.14563468484002948,\n",
       "   'created_at': '2025-10-27T03:15:38.977326-07:00',\n",
       "   'updated_at': '2025-10-27T03:16:02.219811-07:00',\n",
       "   'user_id': 'pradip'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related = memory.search(\"who am i\", user_id=\"pradip\")\n",
    "related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952ed6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': 'a7885191-1db0-45b8-93a2-3a4cdde4a9ff',\n",
       "   'memory': 'Name is Pradip Nichite',\n",
       "   'hash': 'fa942a6331bb89da286d4a9e296d1008',\n",
       "   'metadata': None,\n",
       "   'created_at': '2025-10-27T03:15:38.955725-07:00',\n",
       "   'updated_at': None,\n",
       "   'user_id': 'pradip'},\n",
       "  {'id': 'c1159f6f-9743-4a67-a893-d0aa10b280ee',\n",
       "   'memory': 'Runs FutureSmart AI',\n",
       "   'hash': '68a143a88a3e67ae9ebfb9575bcf49a7',\n",
       "   'metadata': None,\n",
       "   'created_at': '2025-10-27T03:15:38.967325-07:00',\n",
       "   'updated_at': None,\n",
       "   'user_id': 'pradip'},\n",
       "  {'id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       "   'memory': 'Loves building RAG and AI Agent solutions that work in production',\n",
       "   'hash': '93e19ea42b4fd49fa0041025b13429cd',\n",
       "   'metadata': {'category': 'preferences'},\n",
       "   'created_at': '2025-10-27T03:15:38.977326-07:00',\n",
       "   'updated_at': '2025-10-27T03:16:02.219811-07:00',\n",
       "   'user_id': 'pradip'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_memories = memory.get_all(user_id=\"pradip\")\n",
    "all_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce09fc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       " 'memory': 'Loves building RAG and AI Agent solutions that work in production',\n",
       " 'hash': '93e19ea42b4fd49fa0041025b13429cd',\n",
       " 'metadata': {'category': 'preferences'},\n",
       " 'score': None,\n",
       " 'created_at': '2025-10-27T03:15:38.977326-07:00',\n",
       " 'updated_at': '2025-10-27T03:16:02.219811-07:00',\n",
       " 'user_id': 'pradip'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_id = \"c479363c-6dda-494c-85ba-9da670a1cd6c\"\n",
    "memory.get(mem_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0a49fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Memory updated successfully!'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.update(memory_id=mem_id, data=\"Builds custom Gen AI solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae1f7ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3adca25e-4db3-4db5-b619-8c748195c11e',\n",
       "  'memory_id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       "  'old_memory': None,\n",
       "  'new_memory': 'Builds custom AI solutions',\n",
       "  'event': 'ADD',\n",
       "  'created_at': '2025-10-27T03:15:38.977326-07:00',\n",
       "  'updated_at': None,\n",
       "  'is_deleted': False,\n",
       "  'actor_id': None,\n",
       "  'role': None},\n",
       " {'id': '87a348b8-cfdc-4dc4-a3bb-6e80db082b8f',\n",
       "  'memory_id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       "  'old_memory': 'Builds custom AI solutions',\n",
       "  'new_memory': 'Loves building RAG and AI Agent solutions that work in production',\n",
       "  'event': 'UPDATE',\n",
       "  'created_at': '2025-10-27T03:15:38.977326-07:00',\n",
       "  'updated_at': '2025-10-27T03:16:02.219811-07:00',\n",
       "  'is_deleted': False,\n",
       "  'actor_id': None,\n",
       "  'role': None},\n",
       " {'id': 'f3f8bfa0-84b9-4182-ae10-820b051cfea9',\n",
       "  'memory_id': 'c479363c-6dda-494c-85ba-9da670a1cd6c',\n",
       "  'old_memory': 'Loves building RAG and AI Agent solutions that work in production',\n",
       "  'new_memory': 'Builds custom Gen AI solutions',\n",
       "  'event': 'UPDATE',\n",
       "  'created_at': '2025-10-27T03:15:38.977326-07:00',\n",
       "  'updated_at': '2025-10-27T03:19:53.733458-07:00',\n",
       "  'is_deleted': False,\n",
       "  'actor_id': None,\n",
       "  'role': None}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = memory.history(memory_id=mem_id)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ede07",
   "metadata": {},
   "source": [
    "Below we build the simplest possible LangGraph agent that:\n",
    "\n",
    "Accepts user messages.\n",
    "\n",
    "Retrieves relevant memories from Mem0.\n",
    "\n",
    "Injects them into the system prompt for personalised replies.\n",
    "\n",
    "Writes the new interaction back to Mem0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca55f47d",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34590186",
   "metadata": {},
   "source": [
    "### a) Define the shared state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19d13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"Conversation state passed between nodes\"\"\"\n",
    "    messages: Annotated[list[BaseMessage], add_messages]  # chat history for this request\n",
    "    mem0_user_id: str                                     # maps to Mem0 user record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626c329",
   "metadata": {},
   "source": [
    "### b) Init the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92893a19",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from langchain_openai import ChatOpenAI\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_agent\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4.1-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aniln\\miniconda3\\envs\\channel\\Lib\\site-packages\\langchain\\agents\\factory.py:607\u001b[0m, in \u001b[0;36mcreate_agent\u001b[1;34m(model, tools, system_prompt, middleware, response_format, state_schema, context_schema, checkpointer, store, interrupt_before, interrupt_after, debug, name, cache)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# init chat model\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 607\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# Handle tools being None or empty\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aniln\\miniconda3\\envs\\channel\\Lib\\site-packages\\langchain\\chat_models\\base.py:284\u001b[0m, in \u001b[0;36minit_chat_model\u001b[1;34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m has been set but no fields are configurable. Set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    280\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[0;32m    290\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[1;32mc:\\Users\\aniln\\miniconda3\\envs\\channel\\Lib\\site-packages\\langchain\\chat_models\\base.py:311\u001b[0m, in \u001b[0;36m_init_chat_model_helper\u001b[1;34m(model, model_provider, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m     _check_pkg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_openai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    313\u001b[0m     _check_pkg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_anthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aniln\\miniconda3\\envs\\channel\\Lib\\site-packages\\langchain_core\\load\\serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\aniln\\miniconda3\\envs\\channel\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:914\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mAsyncClient(\n\u001b[0;32m    905\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[0;32m    906\u001b[0m         )\n\u001b[0;32m    907\u001b[0m     async_specific \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_async_httpx_client(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: async_api_key_value,\n\u001b[0;32m    913\u001b[0m     }\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_async_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43masync_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_async_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aniln\\miniconda3\\envs\\channel\\Lib\\site-packages\\openai\\_client.py:480\u001b[0m, in \u001b[0;36mAsyncOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    478\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m     )\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e19cb",
   "metadata": {},
   "source": [
    "### c) Create the chatbot node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "def chatbot(state: State):\n",
    "    global memory  # re‚Äëuse the Mem0 instance from Section 1\n",
    "    msgs = state[\"messages\"]\n",
    "    uid = state[\"mem0_user_id\"]\n",
    "\n",
    "    # 1Ô∏è‚É£ Retrieve memories relevant to the latest user msg\n",
    "    mems = memory.search(msgs[-1].content, user_id=uid)\n",
    "    print(f\"Retrieved Memories: {mems}\")\n",
    "\n",
    "    # Build context string\n",
    "    if mems[\"results\"]:\n",
    "        context = \"\".join(f\"- {m['memory']}\" for m in mems[\"results\"])\n",
    "    else:\n",
    "        context = \"No relevant information found.\"\n",
    "\n",
    "    system = SystemMessage(content=f\"\"\"You are a helpful assistant. Use the provided context to personalise your responses.\n",
    "Relevant information from previous conversations:\n",
    "{context}\"\"\")\n",
    "\n",
    "    # 2Ô∏è‚É£ Invoke the LLM\n",
    "    response = llm.invoke([system] + msgs)\n",
    "\n",
    "    # 3Ô∏è‚É£ Persist the new turn\n",
    "    memory.add([\n",
    "        {\"role\": \"user\", \"content\": msgs[-1].content},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ], user_id=uid)\n",
    "\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5c31f",
   "metadata": {},
   "source": [
    "### d) Build & compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4add5df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled successfully ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "print(\"Graph compiled successfully ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7106b",
   "metadata": {},
   "source": [
    "### e) Command‚Äëline loop for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913f1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Memories: {'results': []}\n",
      "ü§ñ Hello Anil! It's great to meet you. As a Senior AI/ML Engineer, you must have a wealth of experience in artificial intelligence and machine learning. How can I assist you today? Are you looking for help with a project, advice on algorithms, or something else?\n",
      "Retrieved Memories: {'results': [{'id': '44bfdb6b-bf59-4ef0-84d4-c90daa692a88', 'memory': 'Is a Senior AI/ML Engineer', 'hash': 'baa183e4c39cb937b08a0cddafe9dc4b', 'metadata': None, 'score': 0.28294593076940633, 'created_at': '2025-10-27T04:34:42.735684-07:00', 'updated_at': None, 'user_id': 'anil'}, {'id': '22b95ece-77ed-4977-a78d-57702e5af8a0', 'memory': 'Name is Anil', 'hash': '96c1f624fd6fb36b8041c9965ee5c044', 'metadata': None, 'score': 0.20380054207995135, 'created_at': '2025-10-27T04:34:42.700597-07:00', 'updated_at': None, 'user_id': 'anil'}]}\n",
      "ü§ñ Your profession is Senior AI/ML Engineer.\n",
      "Retrieved Memories: {'results': [{'id': '22b95ece-77ed-4977-a78d-57702e5af8a0', 'memory': 'Name is Anil', 'hash': '96c1f624fd6fb36b8041c9965ee5c044', 'metadata': None, 'score': 0.155328673932645, 'created_at': '2025-10-27T04:34:42.700597-07:00', 'updated_at': None, 'user_id': 'anil'}, {'id': '44bfdb6b-bf59-4ef0-84d4-c90daa692a88', 'memory': 'Is a Senior AI/ML Engineer', 'hash': 'baa183e4c39cb937b08a0cddafe9dc4b', 'metadata': None, 'score': 0.06477393903012092, 'created_at': '2025-10-27T04:34:42.735684-07:00', 'updated_at': None, 'user_id': 'anil'}]}\n",
      "ü§ñ That's great, Anil! Eating a variety of vegetables is excellent for your health. Do you have any favorite vegetable dishes or cuisines you enjoy? I can also suggest some healthy and delicious vegetable-based recipes if you'd like!\n",
      "Retrieved Memories: {'results': [{'id': 'b57a072b-fcc2-4e9b-b889-2fae014569dd', 'memory': 'Likes to eat vegetables and a variety of foods', 'hash': '700bffa988d395805878a1b2590bea69', 'metadata': None, 'score': 0.4983403768749536, 'created_at': '2025-10-27T04:35:31.648301-07:00', 'updated_at': None, 'user_id': 'anil'}, {'id': '22b95ece-77ed-4977-a78d-57702e5af8a0', 'memory': 'Name is Anil', 'hash': '96c1f624fd6fb36b8041c9965ee5c044', 'metadata': None, 'score': 0.0943566833400311, 'created_at': '2025-10-27T04:34:42.700597-07:00', 'updated_at': None, 'user_id': 'anil'}, {'id': '44bfdb6b-bf59-4ef0-84d4-c90daa692a88', 'memory': 'Is a Senior AI/ML Engineer', 'hash': 'baa183e4c39cb937b08a0cddafe9dc4b', 'metadata': None, 'score': 0.06188087311141701, 'created_at': '2025-10-27T04:34:42.735684-07:00', 'updated_at': None, 'user_id': 'anil'}]}\n",
      "ü§ñ You like to eat vegetables and a variety of foods. If you'd like, I can suggest some new recipes or meal ideas based on your preferences!\n",
      "Retrieved Memories: {'results': [{'id': '22b95ece-77ed-4977-a78d-57702e5af8a0', 'memory': 'Name is Anil', 'hash': '96c1f624fd6fb36b8041c9965ee5c044', 'metadata': None, 'score': 0.26693445289436707, 'created_at': '2025-10-27T04:34:42.700597-07:00', 'updated_at': None, 'user_id': 'anil'}, {'id': '44bfdb6b-bf59-4ef0-84d4-c90daa692a88', 'memory': 'Is a Senior AI/ML Engineer', 'hash': 'baa183e4c39cb937b08a0cddafe9dc4b', 'metadata': None, 'score': 0.1692011235162998, 'created_at': '2025-10-27T04:34:42.735684-07:00', 'updated_at': None, 'user_id': 'anil'}, {'id': 'b57a072b-fcc2-4e9b-b889-2fae014569dd', 'memory': 'Likes to eat vegetables and a variety of foods', 'hash': '700bffa988d395805878a1b2590bea69', 'metadata': None, 'score': 0.13266778121551615, 'created_at': '2025-10-27T04:35:31.648301-07:00', 'updated_at': None, 'user_id': 'anil'}]}\n",
      "ü§ñ Sure, Anil! Here's a summary of your personal details based on what I know:\n",
      "\n",
      "- Name: Anil\n",
      "- Profession: Senior AI/ML Engineer\n",
      "- Interests: Enjoys eating vegetables and a variety of foods\n",
      "\n",
      "If you'd like to add or update any information, just let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def run_conversation(user_input: str, mem0_user_id: str):\n",
    "    state = {\"messages\": [HumanMessage(content=user_input)], \"mem0_user_id\": mem0_user_id}\n",
    "    result = graph.invoke(state)\n",
    "    print(\"ü§ñ\", result[\"messages\"][-1].content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uid = \"anil\"\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() in {\"quit\", \"exit\", \"bye\"}:\n",
    "            break\n",
    "        run_conversation(inp, uid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3be7b5",
   "metadata": {},
   "source": [
    "# 3. Vector‚ÄØDB Setup ‚Äì Configuring Mem0 with Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ba25f",
   "metadata": {},
   "source": [
    "### a) Spin up / locate a Qdrant Cloud cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17af326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the cluster URL and create an API key from the Qdrant dashboard.\n",
    "# # Install the Python client\n",
    "# !pip -q install qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6fa6a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[]\n"
     ]
    }
   ],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "\n",
    "# qdrant = QdrantClient(\n",
    "#     url=\"https://<cluster-id>.<region>.aws.cloud.qdrant.io:6333\",\n",
    "#     api_key=userdata.get(\"Qdrant_API_KEY\")\n",
    "# )\n",
    "# print(qdrant.get_collections())  # sanity‚Äëcheck\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://a92a20e5-49ff-4a82-8365-49bdb11ce639.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.KlvM6TDdSV_v3rZj9hQh5vvsEGOJNrsx8TThjp1N7OA\",\n",
    ")\n",
    "\n",
    "print(qdrant_client.get_collections())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472946e2",
   "metadata": {},
   "source": [
    "### c) Tell Mem0 to use Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37858eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import Memory\n",
    "collection_name = \"mem0_yt\"\n",
    "userdata = {\"Qdrant_API_KEY\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.KlvM6TDdSV_v3rZj9hQh5vvsEGOJNrsx8TThjp1N7OA\"}\n",
    "\n",
    "config = {\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"qdrant\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": collection_name,\n",
    "            \"host\": \"a92a20e5-49ff-4a82-8365-49bdb11ce639.us-east4-0.gcp.cloud.qdrant.io\",\n",
    "            \"port\": 6333,\n",
    "            \"api_key\": userdata.get(\"Qdrant_API_KEY\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "memory = Memory.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe04f25",
   "metadata": {},
   "source": [
    "### d) One‚Äëtime payload index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45709495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mem0 filters by user_id when searching, so Qdrant needs a keyword index on that field. If you skip this step you‚Äôll get:\n",
    "# 400 Bad Request ‚Äì Index required but not found for \"user_id\" of type [keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d4e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=9, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"user_id\",\n",
    "    field_schema=\"keyword\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a667812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': 'e850bfeb-1c25-4b97-bb07-4eca797889c0',\n",
       "   'memory': 'Name is Pradip Nichite',\n",
       "   'event': 'ADD'},\n",
       "  {'id': '48013608-4b18-4820-813e-632d54943d4e',\n",
       "   'memory': 'Runs FutureSmart AI',\n",
       "   'event': 'ADD'},\n",
       "  {'id': '1b567d9e-fc1e-4028-876c-911c8dc799b6',\n",
       "   'memory': 'Loves building RAG and AI Agent solutions for production',\n",
       "   'event': 'ADD'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I'm Pradip Nichite. I run FutureSmart AI.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I love building RAG and AI Agent solutions that work in production.\"}\n",
    "]\n",
    "memory.add(messages, user_id=\"anil\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e71842",
   "metadata": {},
   "source": [
    "### Mem0AI Official  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3440c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mem0ai \n",
    "\n",
    "# mem0AI - api_key - m0-WUPxXSTgQwhjAFTl6USarMszNWKumyvGYEyG0U9j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5259c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import MemoryClient\n",
    "\n",
    "client = MemoryClient(api_key=\"m0-WUPxXSTgQwhjAFTl6USarMszNWKumyvGYEyG0U9j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67ed5ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'message': 'Memory processing has been queued for background execution',\n",
       "   'status': 'PENDING',\n",
       "   'event_id': '8ab4e138-ce39-4b64-a1d6-45d12f346bb5'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": \"Hi, I'm Anil. I'm a vegetarian and I'm allergic to nuts.\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"Hello Anil! I see that you're a vegetarian with a nut allergy.\" }\n",
    "]\n",
    "\n",
    "client.add(messages, user_id=\"anil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19398827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'message': 'Memory processing has been queued for background execution',\n",
       "   'status': 'PENDING',\n",
       "   'event_id': '40dd50c3-ffdc-437f-b51a-b25feb840a80'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": \"I am Senior AI/ML Engineer and working at Optimized Solutions Limited\" },\n",
    "]\n",
    "client.add(messages, user_id=\"anil\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
